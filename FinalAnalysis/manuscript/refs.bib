@article{Sainani_2020, title={Call to increase statistical collaboration in sports science, sport and exercise medicine and sports physiotherapy}, url={http://dx.doi.org/10.1136/bjsports-2020-102607}, DOI={10.1136/bjsports-2020-102607}, journal={British Journal of Sports Medicine}, publisher={BMJ}, author={Sainani, Kristin L and Borg, David N and Caldwell, Aaron R and Butson, Michael L and Tenan, Matthew S and Vickers, Andrew J and Vigotsky, Andrew D and Warmenhoven, John and Nguyen, Robert and Lohse, Keith R. and et al.}, year={2020}, pages={bjsports-2020-102607} } 

@article{Borg_Bon_Sainani_Baguley_Tierney_Drovandi_2020, title={Comment on: ‘Moving Sport and Exercise Science Forward: A Call for the Adoption of More Transparent Research Practices’}, volume={50}, url={http://dx.doi.org/10.1007/s40279-020-01298-5}, DOI={10.1007/s40279-020-01298-5}, number={8}, journal={Sports Medicine}, publisher={Springer Science and Business Media LLC}, author={Borg, David N. and Bon, Joshua J. and Sainani, Kristin L. and Baguley, Brenton J. and Tierney, Nicholas J. and Drovandi, Christopher}, year={2020}, pages={1551–1553} }

@article{Borg_Lohse_Sainani_2020, title={Ten Common Statistical Errors from All Phases of Research, and Their Fixes}, volume={12}, url={http://dx.doi.org/10.1002/pmrj.12395}, DOI={10.1002/pmrj.12395}, number={6}, journal={PM\&R}, publisher={Wiley}, author={Borg, David N. and Lohse, Keith R. and Sainani, Kristin L.}, year={2020}, pages={610–614} }

@misc{Caldwell_Vigotsky_2020, title={Does One Effect Size Fit All? The Case Against Default Effect Sizes for Sport and Exercise Science}, url={http://dx.doi.org/10.31236/osf.io/tfx95}, DOI={10.31236/osf.io/tfx95}, abstractNote={<p>Recent discussions in the sport and exercise science community have focused on the appropriate use and reporting of effect sizes. Sport and exercise scientists often analyze repeated-measures data, from which mean differences are reported. To aid the interpretation of these data, standardized mean differences (SMD) are commonly reported as description of effect size. In this manuscript, we hope to alleviate some confusion. First, we provide a philosophical framework for conceptualizing SMDs; that is, by dichotomizing them into two groups: magnitude-based and signal-to-noise based SMDs. Second, we describe the statistical properties of SMDs and their implications. Finally, we provide high-level recommendations for how sport and exercise scientists can thoughtfully report raw effect sizes, SMDs, or other effect sizes for their own studies. This conceptual framework provides sport and exercise scientists with the background necessary to make and justify their choice of an SMD. The code to reproduce all analyses and figures within the manuscript can be found at the following link: https://www.doi.org/10.17605/OSF.IO/FC5XW.</p>}, publisher={SportRxiv}, author={Caldwell, Aaron R and Vigotsky, Andrew David}, year={2020} } 

@article{Abt_Boreham_Davison_Jackson_Nevill_Wallace_Williams_2020, title={Power, precision, and sample size estimation in sport and exercise science research}, volume={38}, url={http://dx.doi.org/10.1080/02640414.2020.1776002}, DOI={10.1080/02640414.2020.1776002}, number={17}, journal={Journal of Sports Sciences}, publisher={Informa UK Limited}, author={Abt, Grant and Boreham, Colin and Davison, Gareth and Jackson, Robin and Nevill, Alan and Wallace, Eric and Williams, Mark}, year={2020},  pages={1933–1935} } 

@article{Sainani_Lohse_Jones_Vickers_2019, title={Magnitude‐based Inference is not Bayesian and is not a valid method of inference}, volume={29}, url={http://dx.doi.org/10.1111/sms.13491}, DOI={10.1111/sms.13491}, number={9}, journal={Scandinavian Journal of Medicine \& Science in Sports}, publisher={Wiley}, author={Sainani, Kristin L. and Lohse, Keith R. and Jones, Paul Remy and Vickers, Andrew}, year={2019}, pages={1428–1436} } 

@article{buttner_are_2020,
	title = {Are questionable research practices facilitating new discoveries in sport and exercise medicine? {The} proportion of supported hypotheses is implausibly high},
	issn = {1473-0480},
	shorttitle = {Are questionable research practices facilitating new discoveries in sport and exercise medicine?},
	doi = {10.1136/bjsports-2019-101863},
	abstract = {Questionable research practices (QRPs) are intentional and unintentional practices that can occur when designing, conducting, analysing, and reporting research, producing biased study results. Sport and exercise medicine (SEM) research is vulnerable to the same QRPs that pervade the biomedical and psychological sciences, producing false-positive results and inflated effect sizes. Approximately 90\% of biomedical research reports supported study hypotheses, provoking suspicion about the field-wide presence of systematic biases to facilitate study findings that confirm researchers' expectations. In this education review, we introduce three common QRPs (ie, HARKing, P-hacking and Cherry-picking), perform a cross-sectional study to assess the proportion of original SEM research that reports supported study hypotheses, and draw attention to existing solutions and resources to overcome QRPs that manifest in exploratory research. We hypothesised that ≥85\% of original SEM research studies would report supported study hypotheses. Two independent assessors systematically identified, screened, included, and extracted study data from original research articles published between 1 January 2019 and 31 May 2019 in the British Journal of Sports Medicine, Sports Medicine, the American Journal of Sports Medicine, and the Journal of Orthopaedic \& Sports Physical Therapy We extracted data relating to whether studies reported that the primary hypothesis was supported or rejected by the results. Study hypotheses, methodologies, and analysis plans were preregistered at the Open Science Framework. One hundred and twenty-nine original research studies reported at least one study hypothesis, of which 106 (82.2\%) reported hypotheses that were supported by study results. Of 106 studies reporting that primary hypotheses were supported by study results, 75 (70.8\%) studies reported that the primary hypothesis was fully supported by study results. The primary study hypothesis was partially supported by study results in 28 (26.4\%) studies. We detail open science practices and resources that aim to safe-guard against QRPs that bely the credibility and replicability of original research findings.},
	language = {eng},
	journal = {British Journal of Sports Medicine},
	author = {Büttner, Fionn and Toomey, Elaine and McClean, Shane and Roe, Mark and Delahunt, Eamonn},
	year = {2020},
	pmid = {32699001},
	keywords = {education, methodological, research, sport, statistics},
	file = {Snapshot:C\:\\Users\\arcal\\Zotero\\storage\\LEMZNFA6\\bjsports-2019-101863.html:text/html}
}

@article{simmons_false-positive_2011,
	title = {False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant},
	volume = {22},
	issn = {0956-7976},
	shorttitle = {False-{Positive} {Psychology}},
	url = {https://doi.org/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2020-07-22},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	year = {2011},
	note = {Publisher: SAGE Publications Inc},
	keywords = {Adult, Humans, Young Adult, Data Interpretation, Statistical, Computer Simulation, Research Personnel, Statistics as Topic, Research Design, Data Collection, Peer Review, Research, Practice Guidelines as Topic, Publications},
	pages = {1359--1366},
	file = {False-positive psychology\: undisclosed flexibility in data collection and analysis allows presenting anything as significant:C\:\\Users\\arcal\\Zotero\\storage\\FND85BM4\\simmons2011.pdf:application/pdf;Full Text:C\:\\Users\\arcal\\Zotero\\storage\\HRX6Y4WD\\Simmons et al. - 2011 - False-positive psychology undisclosed flexibility.pdf:application/pdf;SAGE PDF Full Text:C\:\\Users\\arcal\\Zotero\\storage\\24BZNVK2\\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:application/pdf}
}

@article{head_extent_2015,
	title = {The extent and consequences of p-hacking in science},
	volume = {13},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106},
	doi = {10.1371/journal.pbio.1002106},
	abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
	language = {en},
	number = {3},
	urldate = {2020-07-22},
	journal = {PLOS Biology},
	author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Publication ethics, Bibliometrics, Reproducibility, Statistical data, Binomials, Medicine and health sciences, Metaanalysis, Test statistics},
	pages = {e1002106},
	file = {Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\UMKGCF4C\\Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf:application/pdf}
}

@article{carp_plurality_2012,
	title = {On the plurality of (methodological) worlds: estimating the analytic flexibility of {fMRI} experiments},
	volume = {6},
	issn = {1662-453X},
	shorttitle = {On the {Plurality} of ({Methodological}) {Worlds}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2012.00149/full},
	doi = {10.3389/fnins.2012.00149},
	abstract = {How likely are published findings in the functional neuroimaging literature to be false? According to a recent mathematical model, the potential for false positives increases with the flexibility of analysis methods. Functional MRI (fMRI) experiments can be analyzed using a large number of commonly used tools, with little consensus on how, when, or whether to apply each one. This situation may lead to substantial variability in analysis outcomes. Thus, the present study sought to estimate the flexibility of neuroimaging analysis by submitting a single event-related fMRI experiment to a large number of unique analysis procedures. Ten analysis steps for which multiple strategies appear in the literature were identified, and two to four strategies were enumerated for each step. Considering all possible combinations of these strategies yielded 6,912 unique analysis pipelines. Activation maps from each pipeline were corrected for multiple comparisons using five thresholding approaches, yielding 34,560 significance maps. While some outcomes were relatively consistent across pipelines, others showed substantial methods-related variability in activation strength, location, and extent. Some analysis decisions contributed to this variability more than others, and different decisions were associated with distinct patterns of variability across the brain. Qualitative outcomes also varied with analysis parameters: many contrasts yielded significant activation under some pipelines but not others. Altogether, these results reveal considerable flexibility in the analysis of fMRI experiments. This observation, when combined with mathematical simulations linking analytic flexibility with elevated false positive rates, suggests that false positive results may be more prevalent than expected in the literature. This risk of inflated false positive rates may be mitigated by constraining the flexibility of analytic choices or by abstaining from selective analysis reporting.},
	language = {English},
	urldate = {2020-07-22},
	journal = {Frontiers in Neuroscience},
	author = {Carp, Joshua},
	year = {2012},
	note = {Publisher: Frontiers},
	keywords = {selective reporting, analysis flexibility, data analysis, false positive results, fMRI},
	file = {Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\KJDVNFKM\\Carp - 2012 - On the Plurality of (Methodological) Worlds Estim.pdf:application/pdf;On the Plurality of (Methodological) Worlds\: Estimating the Analytic Flexibility of fMRI Experiments:C\:\\Users\\arcal\\Zotero\\storage\\RE95PXGG\\carp2012.pdf:application/pdf}
}

@article{kerr_harking_1998,
	title = {{HARKing}: {Hypothesizing} {After} the {Results} are {Known}},
	volume = {2},
	issn = {1088-8683},
	shorttitle = {{HARKing}},
	url = {https://doi.org/10.1207/s15327957pspr0203_4},
	doi = {10.1207/s15327957pspr0203_4},
	abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
	language = {en},
	number = {3},
	urldate = {2020-07-22},
	journal = {Personality and Social Psychology Review},
	author = {Kerr, Norbert L.},
	year = {1998},
	note = {Publisher: SAGE Publications Inc},
	pages = {196--217},
	file = {HARKing\: Hypothesizing After the Results are Known:C\:\\Users\\arcal\\Zotero\\storage\\R5SLERR4\\kerr1998.pdf:application/pdf;SAGE PDF Full Text:C\:\\Users\\arcal\\Zotero\\storage\\DEDACC36\\Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf:application/pdf}
}
@article{Tamminen_Poucher_2018, title={Open science in sport and exercise psychology: Review of current approaches and considerations for qualitative inquiry}, volume={36}, url={http://dx.doi.org/10.1016/j.psychsport.2017.12.010}, DOI={10.1016/j.psychsport.2017.12.010}, journal={Psychology of Sport and Exercise}, publisher={Elsevier BV}, author={Tamminen, Katherine A. and Poucher, Zoë A.}, year={2018},  pages={17–28} } 

@misc{Vigotsky_Nuckols_Heathers_Krieger_Schoenfeld_Steele_2020, title={Improbable data patterns in the work of Barbalho et al.}, url={http://dx.doi.org/10.31236/osf.io/sg3wm}, DOI={10.31236/osf.io/sg3wm}, abstractNote={<p>We describe improbable data patterns in the work of Barbalho et al.</p>}, publisher={SportRxiv}, author={Vigotsky, Andrew David and Nuckols, Gregory Lee and Heathers, James and Krieger, James and Schoenfeld, Brad Jon and Steele, James}, year={2020} } 
@article{halperin_strengtheningthe_2018,
	title = {Strengtheningthe practice of exercise and sport-science research},
	volume = {13},
	issn = {1555-0273},
	doi = {10.1123/ijspp.2017-0322},
	abstract = {Exercise and sport sciences continue to grow as a collective set of disciplines investigating a broad array of basic and applied research questions. Despite the progress, there is room for improvement. A number of problems pertaining to reliability and validity of research practices hinder advancement and the potential impact of the field. These problems include inadequate validation of surrogate outcomes, too few longitudinal and replication studies, limited reporting of null or trivial results, and insufficient scientific transparency. The purpose of this review is to discuss these problems as they pertain to exercise and sport sciences based on their treatment in other disciplines, namely psychology and medicine, and to propose a number of solutions and recommendations.},
	language = {eng},
	number = {2},
	journal = {International Journal of Sports Physiology and Performance},
	author = {Halperin, Israel and Vigotsky, Andrew D. and Foster, Carl and Pyne, David B.},
	year = {2018},
	pmid = {28787228},
	keywords = {Humans, methodology, Reproducibility of Results, Research Design, Exercise, Sports, null results, replication},
	pages = {127--134},
	file = {Full Text:C\:\\Users\\arcal\\Zotero\\storage\\XHRA7HPW\\Halperin et al. - 2018 - Strengthening the Practice of Exercise and Sport-S.pdf:application/pdf;Strengthening the Practice of Exercise and Sport-Science Research:C\:\\Users\\arcal\\Zotero\\storage\\228HJ8CJ\\halperin2017.pdf:application/pdf}
}

@article{lohse_underpowered_2016,
	title = {Underpowered and overworked: problems with data analysis in motor learning studies},
	volume = {4},
	issn = {2325-3193, 2325-3215},
	shorttitle = {Underpowered and {Overworked}},
	url = {https://journals.humankinetics.com/view/journals/jmld/4/1/article-p37.xml},
	doi = {10.1123/jmld.2015-0010},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}p{\textgreater}Appropriate statistical analysis is essential for accurate and reliable research. Statistical practices have an immediate impact on the perceived results of a single study but also remote effects on the dissemination of information among scientists and the cumulative nature of research. To accurately quantify potential problems facing the field of motor learning, we systematically reviewed publications from seven journals over the past 2 years to find experiments that tested the effects of different training conditions on delayed retention and transfer tests (i.e., classic motor learning paradigms). Eighteen studies were included. These studies had small sample sizes ({\textless}em{\textgreater}Mdn n{\textless}/em{\textgreater}/group = 11.00, interquartile range [{\textless}em{\textgreater}IQR{\textless}/em{\textgreater}]= 9.6–15.5), multiple dependent variables ({\textless}em{\textgreater}Mdn{\textless}/em{\textgreater} = 2, IQR = 2–4), and many statistical tests per article ({\textless}em{\textgreater}Mdn{\textless}/em{\textgreater} = 83.5, {\textless}em{\textgreater}IQR{\textless}/em{\textgreater} = 55.8–112.5). The observed effect sizes were large ({\textless}em{\textgreater}d{\textless}/em{\textgreater} = 0.71, {\textless}em{\textgreater}IQR{\textless}/em{\textgreater} = 0.49, 1.11). However, the distribution of effect sizes was biased, {\textless}em{\textgreater}t{\textless}/em{\textgreater}(16) = 3.48, {\textless}em{\textgreater}p{\textless}/em{\textgreater} \&lt; .01. These metadata indicate problems with the way motor learning research is conducted (or at least published). We recommend several potential solutions to address these issues: a priori power calculations, prespecified analyses, data sharing, and dissemination of null results. Furthermore, we hope these data will spark serious action from all stakeholders (researchers, editorial boards, and publishers) in the field.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {en\_US},
	number = {1},
	urldate = {2020-07-22},
	journal = {Journal of Motor Learning and Development},
	author = {Lohse, Keith and Buchanan, Taylor and Miller, Matthew},
	year = {2016},
	note = {Publisher: Human Kinetics, Inc.
Section: Journal of Motor Learning and Development},
	pages = {37--58},
	file = {Underpowered and Overworked\: Problems With Data Analysis in Motor Learning Studies:C\:\\Users\\arcal\\Zotero\\storage\\NAPUKXT6\\lohse2016.pdf:application/pdf}
}

@article{bernards_current_2017,
	title = {Current research and statistical practices in sport science and a need for change},
	volume = {5},
	issn = {2075-4663},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5969020/},
	doi = {10.3390/sports5040087},
	abstract = {Current research ideologies in sport science allow for the possibility of investigators producing statistically significant results to help fit the outcome into a predetermined theory. Additionally, under the current Neyman-Pearson statistical structure, some argue that null hypothesis significant testing (NHST) under the frequentist approach is flawed, regardless. For example, a p-value is unable to measure the probability that the studied hypothesis is true, unable to measure the size of an effect or the importance of a result, and unable to provide a good measure of evidence regarding a model or hypothesis. Many of these downfalls are key questions researchers strive to answer following an investigation. Therefore, a shift towards a magnitude-based inference model, and eventually a fully Bayesian framework, is thought to be a better fit from a statistical standpoint and may be an improved way to address biases within the literature. The goal of this article is to shed light on the current research and statistical shortcomings the field of sport science faces today, and offer potential solutions to help guide future research practices.},
	number = {4},
	urldate = {2020-07-22},
	journal = {Sports},
	author = {Bernards, Jake R. and Sato, Kimitake and Haff, G. Gregory and Bazyler, Caleb D.},
	year = {2017},
	pmid = {29910447},
	pmcid = {PMC5969020},
	file = {Current Research and Statistical Practices in Sport Science and a Need for Change:C\:\\Users\\arcal\\Zotero\\storage\\Q6U8GAI7\\bernards2017.pdf:application/pdf;PubMed Central Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\SAWA6C5X\\Bernards et al. - 2017 - Current Research and Statistical Practices in Spor.pdf:application/pdf}
}

@article{fanelli_how_2009,
	title = {How many scientists fabricate and falsify research? {A} systematic review and meta-analysis of survey data},
	volume = {4},
	issn = {1932-6203},
	shorttitle = {How many scientists fabricate and falsify research?},
	doi = {10.1371/journal.pone.0005738},
	abstract = {The frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys. To standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, "cooking" of data, etc... Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis. A pooled weighted average of 1.97\% (N = 7, 95\%CI: 0.86-4.45) of scientists admitted to have fabricated, falsified or modified data or results at least once--a serious form of misconduct by any standard--and up to 33.7\% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12\% (N = 12, 95\% CI: 9.91-19.72) for falsification, and up to 72\% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words "falsification" or "fabrication", and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others. Considering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.},
	language = {eng},
	number = {5},
	journal = {PloS One},
	author = {Fanelli, Daniele},
	year = {2009},
	pmid = {19478950},
	pmcid = {PMC2685008},
	keywords = {Regression Analysis, Research Personnel, Data Collection, Scientific Misconduct, Ethics, Research},
	pages = {e5738},
	file = {Full Text:C\:\\Users\\arcal\\Zotero\\storage\\8MBRNE3S\\Fanelli - 2009 - How many scientists fabricate and falsify research.pdf:application/pdf;How many scientists fabricate and falsify research? A systematic review and meta-analysis of survey data:C\:\\Users\\arcal\\Zotero\\storage\\4C2UG8F2\\fanelli2009.pdf:application/pdf}
}

@article{ioannidis_why_2005,
	title = {Why most published research findings are false},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2020-07-22},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Metaanalysis, Cancer risk factors, Finance, Genetic epidemiology, Genetics of disease, Research design, Schizophrenia},
	pages = {e124},
	file = {Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\WFYJ3KT4\\Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf;Why Most Published Research Findings Are False:C\:\\Users\\arcal\\Zotero\\storage\\4ZEXB7S4\\ioannidis2005.pdf:application/pdf}
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	copyright = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	language = {en},
	number = {1},
	urldate = {2020-07-22},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--9},
	file = {A manifesto for reproducible science:C\:\\Users\\arcal\\Zotero\\storage\\IPXPKSEH\\10.1038@s41562-016-0021.pdf:application/pdf;Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\54JPIT7I\\Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf}
}

@article{nosek_preregistration_2018,
	title = {The preregistration revolution},
	volume = {115},
	issn = {1091-6490},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes-a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	language = {eng},
	number = {11},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	year = {2018},
	pmid = {29531091},
	pmcid = {PMC5856500},
	keywords = {Humans, methodology, Predictive Value of Tests, Science, open science, Research, confirmatory analysis, exploratory analysis, preregistration, Laboratory Personnel, Workforce},
	pages = {2600--2606},
	file = {Full Text:C\:\\Users\\arcal\\Zotero\\storage\\ALDEDVWP\\Nosek et al. - 2018 - The preregistration revolution.pdf:application/pdf;The preregistration revolution:C\:\\Users\\arcal\\Zotero\\storage\\UKB65IET\\nosek2018.pdf:application/pdf}
}

@article{cristea_p_2018,
	title = {P values in display items are ubiquitous and almost invariably significant: {A} survey of top science journals},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {P values in display items are ubiquitous and almost invariably significant},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5953482/},
	doi = {10.1371/journal.pone.0197440},
	abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
	number = {5},
	urldate = {2020-07-22},
	journal = {PLOS ONE},
	author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
	year = {2018},
	pmid = {29763472},
	pmcid = {PMC5953482},
	file = {P values in display items are ubiquitous and almost invariably significant\: A survey of top science journals:C\:\\Users\\arcal\\Zotero\\storage\\XAM7RXLS\\cristea2018.pdf:application/pdf;PubMed Central Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\LZEHX4L3\\Cristea and Ioannidis - 2018 - P values in display items are ubiquitous and almos.pdf:application/pdf}
}

@article{chambers_registered_2015,
	title = {Registered reports: realigning incentives in scientific publishing},
	volume = {66},
	issn = {1973-8102},
	shorttitle = {Registered reports},
	doi = {10.1016/j.cortex.2015.03.022},
	language = {eng},
	journal = {Cortex},
	author = {Chambers, Christopher D. and Dienes, Zoltan and McIntosh, Robert D. and Rotshtein, Pia and Willmes, Klaus},
	year = {2015},
	pmid = {25892410},
	keywords = {Humans, Reproducibility of Results, Biomedical Research, Publishing, Peer Review, Research, Editorial Policies, Motivation, Publication Bias},
	pages = {A1--2},
	file = {Accepted Version:C\:\\Users\\arcal\\Zotero\\storage\\ZN94L4EN\\Chambers et al. - 2015 - Registered reports realigning incentives in scien.pdf:application/pdf;Registered reports\: realigning incentives in scientific publishing:C\:\\Users\\arcal\\Zotero\\storage\\TF8KQZVY\\chambers2015.pdf:application/pdf}
}

@article{makel_replications_2012,
	title = {Replications in psychology research: how often do they really occur?},
	volume = {7},
	issn = {1745-6916},
	shorttitle = {Replications in {Psychology} {Research}},
	doi = {10.1177/1745691612460688},
	abstract = {Recent controversies in psychology have spurred conversations about the nature and quality of psychological research. One topic receiving substantial attention is the role of replication in psychological science. Using the complete publication history of the 100 psychology journals with the highest 5-year impact factors, the current article provides an overview of replications in psychological research since 1900. This investigation revealed that roughly 1.6\% of all psychology publications used the term replication in text. A more thorough analysis of 500 randomly selected articles revealed that only 68\% of articles using the term replication were actual replications, resulting in an overall replication rate of 1.07\%. Contrary to previous findings in other fields, this study found that the majority of replications in psychology journals reported similar findings to their original studies (i.e., they were successful replications). However, replications were significantly less likely to be successful when there was no overlap in authorship between the original and replicating articles. Moreover, despite numerous systemic biases, the rate at which replications are being published has increased in recent decades.},
	language = {eng},
	number = {6},
	journal = {Perspectives on Psychological Science},
	author = {Makel, Matthew C. and Plucker, Jonathan A. and Hegarty, Boyd},
	year = {2012},
	pmid = {26168110},
	keywords = {replication, content analysis, research methodology},
	pages = {537--542},
	file = {Full Text:C\:\\Users\\arcal\\Zotero\\storage\\24TQCYTV\\Makel et al. - 2012 - Replications in Psychology Research How Often Do .pdf:application/pdf;Replications in Psychology Research\: How Often Do They Really Occur?:C\:\\Users\\arcal\\Zotero\\storage\\F9EHEG4V\\makel2012.pdf:application/pdf}
}

@article{simons_introduction_2014,
	title = {An introduction to registered replication reports at perspectives on psychological science},
	volume = {9},
	issn = {1745-6924},
	doi = {10.1177/1745691614543974},
	language = {eng},
	number = {5},
	journal = {Perspectives on Psychological Science},
	author = {Simons, Daniel J. and Holcombe, Alex O. and Spellman, Barbara A.},
	year = {2014},
	pmid = {26186757},
	keywords = {Humans, Statistics as Topic, Research Design, Periodicals as Topic, Editorial Policies, Psychology},
	pages = {552--555},
	file = {An Introduction to Registered Replication Reports at Perspectives on Psychological Science:C\:\\Users\\arcal\\Zotero\\storage\\I46XQW94\\simons2014.pdf:application/pdf}
}

@article{assen_why_2014,
	title = {Why publishing everything is more effective than selective publishing of statistically significant results},
	volume = {9},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0084896},
	doi = {10.1371/journal.pone.0084896},
	abstract = {Background De Winter and Happee [1] examined whether science based on selective publishing of significant results may be effective in accurate estimation of population effects, and whether this is even more effective than a science in which all results are published (i.e., a science without publication bias). Based on their simulation study they concluded that “selective publishing yields a more accurate meta-analytic estimation of the true effect than publishing everything, (and that) publishing nonreplicable results while placing null results in the file drawer can be beneficial for the scientific collective” (p.4). Methods and Findings Using their scenario with a small to medium population effect size, we show that publishing everything is more effective for the scientific collective than selective publishing of significant results. Additionally, we examined a scenario with a null effect, which provides a more dramatic illustration of the superiority of publishing everything over selective publishing. Conclusion Publishing everything is more effective than only reporting significant outcomes.},
	language = {en},
	number = {1},
	urldate = {2020-07-22},
	journal = {PLOS ONE},
	author = {Assen, Marcel A. L. M. van and Aert, Robbie C. M. van and Nuijten, Michèle B. and Wicherts, Jelte M.},
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Publication ethics, Scientists, Scientific publishing, Normal distribution, Metaanalysis, Simulation and modeling, Social sciences, Statistical methods},
	pages = {e84896},
	file = {Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\E87NNC7C\\Assen et al. - 2014 - Why Publishing Everything Is More Effective than S.pdf:application/pdf;Why Publishing Everything Is More Effective than Selective Publishing of Statistically Significant Results:C\:\\Users\\arcal\\Zotero\\storage\\WCCHRMSK\\vanassen2014.pdf:application/pdf}
}

@article{mahoney_publication_1977,
	title = {Publication prejudices: an experimental study of confirmatory bias in the peer review system},
	volume = {1},
	issn = {1573-2819},
	shorttitle = {Publication prejudices},
	url = {https://doi.org/10.1007/BF01173636},
	doi = {10.1007/BF01173636},
	abstract = {Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.},
	language = {en},
	number = {2},
	urldate = {2020-07-22},
	journal = {Cognitive Therapy and Research},
	author = {Mahoney, Michael J.},
	year = {1977},
	pages = {161--175},
	file = {Publication prejudices\: An experimental study of confirmatory bias in the peer review system:C\:\\Users\\arcal\\Zotero\\storage\\YIHY9449\\mahoney1977.pdf:application/pdf;Springer Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\X437MR4W\\Mahoney - 1977 - Publication prejudices An experimental study of c.pdf:application/pdf}
}

@article{caldwell_moving_2020,
	title = {Moving sport and exercise science forward: a call for the adoption of more transparent research practices},
	volume = {50},
	issn = {1179-2035},
	shorttitle = {Moving {Sport} and {Exercise} {Science} {Forward}},
	url = {https://doi.org/10.1007/s40279-019-01227-1},
	doi = {10.1007/s40279-019-01227-1},
	abstract = {The primary means of disseminating sport and exercise science research is currently through journal articles. However, not all studies, especially those with null findings, make it to formal publication. This publication bias towards positive findings may contribute to questionable research practices. Preregistration is a solution to prevent the publication of distorted evidence resulting from this system. This process asks authors to register their hypotheses and methods before data collection on a publicly available repository or by submitting a Registered Report. In the Registered Report format, authors submit a stage 1 manuscript to a participating journal that includes an introduction, methods, and any pilot data indicating the exploratory or confirmatory nature of the study. After a stage 1 peer review, the manuscript can then be offered in-principle acceptance, rejected, or sent back for revisions to improve the quality of the study. If accepted, the project is guaranteed publication, assuming the authors follow the data collection and analysis protocol. After data collection, authors re-submit a stage 2 manuscript that includes the results and discussion, and the study is evaluated on clarity and conformity with the planned analysis. In its final form, Registered Reports appear almost identical to a typical publication, but give readers confidence that the hypotheses and main analyses are less susceptible to bias from questionable research practices. From this perspective, we argue that inclusion of Registered Reports by researchers and journals will improve the transparency, replicability, and trust in sport and exercise science research. The preprint version of this work is available on SportR\$\${\textbackslash}chi \$\$χiv: https://osf.io/preprints/sportrxiv/fxe7a/.},
	language = {en},
	number = {3},
	urldate = {2020-07-22},
	journal = {Sports Medicine},
	author = {Caldwell, Aaron R. and Vigotsky, Andrew D. and Tenan, Matthew S. and Radel, Rémi and Mellor, David T. and Kreutzer, Andreas and Lahart, Ian M. and Mills, John P. and Boisgontier, Matthieu P. and Boardley, Ian and Bouza, Brooke and Cheval, Boris and Chow, Zad Rafi and Contreras, Bret and Dieter, Brad and Halperin, Israel and Haun, Cody and Knudson, Duane and Lahti, Johan and Miller, Matthew and Morin, Jean-Benoit and Naughton, Mitchell and Neva, Jason and Nuckols, Greg and Peters, Sue and Roberts, Brandon and Rosa-Caldwell, Megan and Schmidt, Julia and Schoenfeld, Brad J. and Severin, Richard and Skarabot, Jakob and Steele, James and Twomey, Rosie and Zenko, Zachary and Lohse, Keith R. and Nunan, David and {Consortium for Transparency in Exercise Science (COTES) Collaborators}},
	year = {2020},
	pages = {449--459},
	file = {Moving Sport and Exercise Science Forward\: A Call for the Adoption of More Transparent Research Practices:C\:\\Users\\arcal\\Zotero\\storage\\JEQT7UNZ\\10.1007@s40279-019-01227-1.pdf:application/pdf}
}

@article{kaplan_likelihood_2015,
	title = {Likelihood of null effects of large {NHLBI} clinical trials has increased over time},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382},
	doi = {10.1371/journal.pone.0132382},
	abstract = {Background We explore whether the number of null results in large National Heart Lung, and Blood Institute (NHLBI) funded trials has increased over time. Methods We identified all large NHLBI supported RCTs between 1970 and 2012 evaluating drugs or dietary supplements for the treatment or prevention of cardiovascular disease. Trials were included if direct costs {\textgreater}\$500,000/year, participants were adult humans, and the primary outcome was cardiovascular risk, disease or death. The 55 trials meeting these criteria were coded for whether they were published prior to or after the year 2000, whether they registered in clinicaltrials.gov prior to publication, used active or placebo comparator, and whether or not the trial had industry co-sponsorship. We tabulated whether the study reported a positive, negative, or null result on the primary outcome variable and for total mortality. Results 17 of 30 studies (57\%) published prior to 2000 showed a significant benefit of intervention on the primary outcome in comparison to only 2 among the 25 (8\%) trials published after 2000 (χ2=12.2,df= 1, p=0.0005). There has been no change in the proportion of trials that compared treatment to placebo versus active comparator. Industry co-sponsorship was unrelated to the probability of reporting a significant benefit. Pre-registration in clinical trials.gov was strongly associated with the trend toward null findings. Conclusions The number NHLBI trials reporting positive results declined after the year 2000. Prospective declaration of outcomes in RCTs, and the adoption of transparent reporting standards, as required by clinicaltrials.gov, may have contributed to the trend toward null findings.},
	language = {en},
	number = {8},
	urldate = {2020-07-22},
	journal = {PLOS ONE},
	author = {Kaplan, Robert M. and Irvin, Veronica L.},
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Randomized controlled trials, Cardiovascular diseases, Cardiovascular therapy, Coronary heart disease, Drug therapy, Myocardial infarction, Sudden cardiac death, Women's health},
	pages = {e0132382},
	file = {Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\2ERSZQCR\\Kaplan and Irvin - 2015 - Likelihood of Null Effects of Large NHLBI Clinical.pdf:application/pdf;Likelihood of Null Effects of Large NHLBI Clinical Trials Has Increased over Time:C\:\\Users\\arcal\\Zotero\\storage\\LMS2BYBR\\kaplan2015.pdf:application/pdf}
}

@article{jonas_how_2016,
	title = {How can preregistration contribute to research in our field?},
	volume = {1},
	issn = {2374-3603},
	url = {https://doi.org/10.1080/23743603.2015.1070611},
	doi = {10.1080/23743603.2015.1070611},
	abstract = {Comprehensive Results in Social Psychology (CRSP) is a novel journal for preregistered research (so-called registered reports, RR) in the field of social psychology. It offers RR-only publications, with the possibility of adding exploratory analysis and data as well. After submission of introduction, hypotheses, methods, procedure, and analysis plan, submitted manuscripts are reviewed prior to data collection. If the peer review process results in a positive evaluation of the manuscript, an initial publication agreement (IPA) is issued upon which publication of the manuscript (given adherence to the registered protocol) independent of the obtained results is possible. CRSP seeks to complement the publication options in our field by making transparent confirmatory and exploratory research possible.},
	number = {1-3},
	urldate = {2020-07-22},
	journal = {Comprehensive Results in Social Psychology},
	author = {Jonas, Kai J. and Cesario, Joseph},
	year = {2016},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/23743603.2015.1070611},
	keywords = {replication, open science, Pre-registration, social psychology},
	pages = {1--7},
	file = {How can preregistration contribute to research in our field?:C\:\\Users\\arcal\\Zotero\\storage\\28WMYS2J\\jonas2015.pdf:application/pdf}
}

@article{collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
	language = {en},
	number = {6251},
	urldate = {2020-07-22},
	journal = {Science},
	author = {Collaboration, Open Science},
	year = {2015},
	pmid = {26315443},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	file = {Estimating the reproducibility of psychological science:C\:\\Users\\arcal\\Zotero\\storage\\WRAW92J7\\estimating-the-reproducibility-of-psychological-science-2015.pdf:application/pdf;Full Text:C\:\\Users\\arcal\\Zotero\\storage\\BGCIIACL\\Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf}
}

@article{rasmussen_association_2009,
	title = {Association of trial registration with the results and conclusions of published trials of new oncology drugs},
	volume = {10},
	issn = {1745-6215},
	url = {https://doi.org/10.1186/1745-6215-10-116},
	doi = {10.1186/1745-6215-10-116},
	abstract = {Registration of clinical trials has been introduced largely to reduce bias toward statistically significant results in the trial literature. Doubts remain about whether advance registration alone is an adequate measure to reduce selective publication, selective outcome reporting, and biased design. One of the first areas of medicine in which registration was widely adopted was oncology, although the bulk of registered oncology trials remain unpublished. The net influence of registration on the literature remains untested. This study compares the prevalence of favorable results and conclusions among published reports of registered and unregistered randomized controlled trials of new oncology drugs.},
	number = {1},
	urldate = {2020-07-22},
	journal = {Trials},
	author = {Rasmussen, Nicolas and Lee, Kirby and Bero, Lisa},
	year = {2009},
	pages = {116},
	file = {Association of trial registration with the results and conclusions of published trials of new oncology drugs:C\:\\Users\\arcal\\Zotero\\storage\\PFE7CIPN\\rasmussen2009.pdf:application/pdf;Full Text:C\:\\Users\\arcal\\Zotero\\storage\\YH8W38P8\\Rasmussen et al. - 2009 - Association of trial registration with the results.pdf:application/pdf}
}

@article{sterling_publication_1995,
	title = {Publication decisions revisited: the effect of the outcome of statistical tests on the decision to publish and vice versa},
	volume = {49},
	issn = {0003-1305},
	shorttitle = {Publication {Decisions} {Revisited}},
	url = {https://www.jstor.org/stable/2684823},
	doi = {10.2307/2684823},
	abstract = {This article presents evidence that published results of scientific investigations are not a representative sample of results of all scientific studies. Research studies from 11 major journals demonstrate the existence of biases that favor studies that observe effects that, on statistical evaluation, have a low probability of erroneously rejecting the so-called null hypothesis (H0). This practice makes the probability of erroneously rejecting H0 different for the reader than for the investigator. It introduces two biases in the interpretation of the scientific literature: one due to multiple repetition of studies with false hypothesis, and one due to failure to publish smaller and less significant outcomes of tests of a true hypotheses. These practices distort the results of literature surveys and of meta-analyses. These results also indicate that practice leading to publication bias have not changed over a period of 30 years},
	number = {1},
	urldate = {2020-07-22},
	journal = {The American Statistician},
	author = {Sterling, T. D. and Rosenbaum, W. L. and Weinkam, J. J.},
	year = {1995},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {108--112},
	file = {Publication Decisions Revisited\: The Effect of the Outcome of Statistical Tests on the Decision to Publish and Vice Versa:C\:\\Users\\arcal\\Zotero\\storage\\2973BLH3\\sterling1995.pdf:application/pdf}
}

@article{wiseman_registered_2019,
	title = {Registered reports: an early example and analysis},
	volume = {7},
	issn = {2167-8359},
	shorttitle = {Registered reports},
	doi = {10.7717/peerj.6232},
	abstract = {The recent 'replication crisis' in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting 'Registered Reports', wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson's pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
	language = {eng},
	journal = {PeerJ},
	author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
	year = {2019},
	pmid = {30671302},
	pmcid = {PMC6339469},
	keywords = {Psychology, Replication, Publication bias, Methodology, Registered reports},
	pages = {e6232},
	file = {Full Text:C\:\\Users\\arcal\\Zotero\\storage\\7HDZEUPS\\Wiseman et al. - 2019 - Registered reports an early example and analysis.pdf:application/pdf}
}

@article{rosenthal_file_1979,
	title = {The file drawer problem and tolerance for null results},
	volume = {86},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/0033-2909.86.3.638},
	abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	year = {1979},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Experimentation, Scientific Communication, Statistical Probability, Statistical Tests, Type I Errors},
	pages = {638--641},
	file = {The file drawer problem and tolerance for null results:C\:\\Users\\arcal\\Zotero\\storage\\TX58MNKT\\rosenthal1979.pdf:application/pdf}
}

@article{allen_open_2019,
	title = {Open science challenges, benefits and tips in early career and beyond},
	volume = {17},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246},
	doi = {10.1371/journal.pbio.3000246},
	abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
	language = {en},
	number = {5},
	urldate = {2020-07-22},
	journal = {PLOS Biology},
	author = {Allen, Christopher and Mehler, David M. A.},
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Reproducibility, Statistical data, Open data, Open science, Careers, Experimental design, Neuroimaging, Peer review},
	pages = {e3000246},
	file = {Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\PV7B58NV\\Allen and Mehler - 2019 - Open science challenges, benefits and tips in earl.pdf:application/pdf;Open science challenges, benefits and tips in early career and beyond:C\:\\Users\\arcal\\Zotero\\storage\\UPT9Y3NU\\allen2019.pdf:application/pdf}
}

@article{cleophas_is_1996,
	title = {Is selective reporting of well-designed clinical research unethical as well as unscientific?},
	volume = {140},
	issn = {0028-2162},
	language = {eng},
	number = {9},
	journal = {Nederlands Tijdschrift Voor Geneeskunde},
	author = {Cleophas, T. J.},
	year = {1996},
	pmid = {8628443},
	keywords = {Humans, Clinical Trials as Topic, Communication, Ethics, Professional},
	pages = {509--510}
}

@article{fanelli_positive_2010,
	title = {“{Positive}” results increase down the hierarchy of the sciences},
	volume = {5},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010068},
	doi = {10.1371/journal.pone.0010068},
	abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the “hardness” of scientific research—i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors—is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a “positive” (full or partial) or “negative” support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in “softer” sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
	language = {en},
	number = {4},
	urldate = {2020-07-22},
	journal = {PLOS ONE},
	author = {Fanelli, Daniele},
	year = {2010},
	note = {Publisher: Public Library of Science},
	keywords = {Scientists, Social research, Mental health and psychiatry, Social sciences, Forecasting, Physical sciences, Social psychology, Sociology},
	pages = {e10068},
	file = {“Positive” Results Increase Down the Hierarchy of the Sciences:C\:\\Users\\arcal\\Zotero\\storage\\E4D2SVL4\\fanelli2010.pdf:application/pdf;Full Text PDF:C\:\\Users\\arcal\\Zotero\\storage\\JXH956UN\\Fanelli - 2010 - “Positive” Results Increase Down the Hierarchy of .pdf:application/pdf}
}

@article{scheel_excess_2020,
	title = {An excess of positive results: comparing the standard psychology literature with registered reports},
	shorttitle = {An excess of positive results},
	url = {https://osf.io/p6e9c},
	doi = {10.31234/osf.io/p6e9c},
	abstract = {When studies with positive results that support the tested hypotheses have a higher probability of being published than studies with negative results, the literature will give a distorted view of the evidence for scientiﬁc claims. Psychological scientists have been concerned about the degree of distortion in their literature due to publication bias and inﬂated Type-1 error rates. Registered Reports were developed with the goal to minimise such biases: In this new publication format, peer review and the decision to publish take place before the study results are known. We compared the results in the full population of published Registered Reports in Psychology (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) by searching 633 journals for the phrase ‘test* the hypothes*’ (replicating a method by Fanelli, 2010). Analysing the ﬁrst hypothesis reported in each paper, we found 96\% positive results in standard reports, but only 44\% positive results in Registered Reports. The diﬀerence remained nearly as large when direct replications were excluded from the analysis (96\% vs 50\% positive results). This large gap suggests that psychologists underreport negative results to an extent that threatens cumulative science. Although our study did not directly test the eﬀectiveness of Registered Reports at reducing bias, these results show that the introduction of Registered Reports has led to a much larger proportion of negative results appearing in the published literature compared to standard reports.},
	language = {en},
	urldate = {2020-07-22},
	journal = {PsyArXiv},
	author = {Scheel, Anne M. and Schijen, Mitchell and Lakens, Daniel},
	year = {2020},
	doi = {10.31234/osf.io/p6e9c},
	file = {Scheel et al. - 2020 - An excess of positive results Comparing the stand.pdf:C\:\\Users\\arcal\\Zotero\\storage\\FAHEZ7P8\\Scheel et al. - 2020 - An excess of positive results Comparing the stand.pdf:application/pdf}
}

@article{fanelli_negative_2012,
	title = {Negative results are disappearing from most disciplines and countries},
	volume = {90},
	url = {https://ideas.repec.org/a/spr/scient/v90y2012i3d10.1007_s11192-011-0494-7.html},
	abstract = {Abstract Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have “tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
	language = {en},
	number = {3},
	urldate = {2020-07-22},
	journal = {Scientometrics},
	author = {Fanelli, Daniele},
	year = {2012},
	note = {Publisher: Springer \& Akadémiai Kiadó},
	keywords = {Bias, Competition, Misconduct, Publication, Publish or perish, Research evaluation},
	pages = {891--904}
}

@misc{NosekErrington2019, title={What is replication?}, url={http://dx.doi.org/10.31222/osf.io/u4g6t}, DOI={10.31222/osf.io/u4g6t}, abstractNote={<p>Credibility of scientific claims is established with evidence for their replicability using new data. According to common understanding, replication is repeating a study’s procedure and observing whether the prior finding recurs. This definition is intuitive, easy to apply, and incorrect. We propose that replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research. This definition reduces emphasis on operational characteristics of the study and increases emphasis on the interpretation of possible outcomes. The purpose of replication is to advance theory by confronting existing understanding with new evidence. Ironically, the value of replication may be strongest when existing understanding is weakest. Successful replication provides evidence of generalizability across the conditions that inevitably differ from the original study; Unsuccessful replication indicates that the reliability of the finding may be more constrained than recognized previously. Defining replication as a confrontation of current theoretical expectations clarifies its important, exciting, and generative role in scientific progress.</p>}, publisher={Center for Open Science}, author={Nosek, Brian A. and Errington, Timothy M.}, year={2019}} 

@article{John_Loewenstein_Prelec_2012, title={Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling}, volume={23}, url={http://dx.doi.org/10.1177/0956797611430953}, DOI={10.1177/0956797611430953}, number={5}, journal={Psychological Science}, publisher={SAGE Publications}, author={John, Leslie K. and Loewenstein, George and Prelec, Drazen}, year={2012}, pages={524–532} } 

@article{Boekel_Wagenmakers_Belay_Verhagen_Brown_Forstmann_2015, title={A purely confirmatory replication study of structural brain-behavior correlations}, volume={66}, url={http://dx.doi.org/10.1016/j.cortex.2014.11.019}, DOI={10.1016/j.cortex.2014.11.019}, journal={Cortex}, publisher={Elsevier BV}, author={Boekel, Wouter and Wagenmakers, Eric-Jan and Belay, Luam and Verhagen, Josine and Brown, Scott and Forstmann, Birte U.}, year={2015}, pages={115–133} } 

@article{Turner_Paul_Miller_Barbey_2018, title={Small sample sizes reduce the replicability of task-based fMRI studies}, volume={1}, url={http://dx.doi.org/10.1038/s42003-018-0073-z}, DOI={10.1038/s42003-018-0073-z}, number={1}, journal={Communications Biology}, publisher={Springer Science and Business Media LLC}, author={Turner, Benjamin O. and Paul, Erick J. and Miller, Michael B. and Barbey, Aron K.}, year={2018}} 

@article{Kharabian_Genon_2019, title={Empirical examination of the replicability of associations between brain structure and psychological variables}, volume={8}, url={http://dx.doi.org/10.7554/eLife.43464}, DOI={10.7554/elife.43464}, abstractNote={<jats:p>Linking interindividual differences in psychological phenotype to variations in brain structure is an old dream for psychology and a crucial question for cognitive neurosciences. Yet, replicability of the previously-reported ‘structural brain behavior’ (SBB)-associations has been questioned, recently. Here, we conducted an empirical investigation, assessing replicability of SBB among heathy adults. For a wide range of psychological measures, the replicability of associations with gray matter volume was assessed. Our results revealed that among healthy individuals 1) finding an association between performance at standard psychological tests and brain morphology is relatively unlikely 2) significant associations, found using an exploratory approach, have overestimated effect sizes and 3) can hardly be replicated in an independent sample. After considering factors such as sample size and comparing our findings with more replicable SBB-associations in a clinical cohort and replicable associations between brain structure and non-psychological phenotype, we discuss the potential causes and consequences of these findings.</jats:p>}, journal={eLife}, publisher={eLife Sciences Publications, Ltd}, author={Kharabian Masouleh, Shahrzad and Eickhoff, Simon B and Hoffstaedter, Felix and Genon, Sarah}, year={2019}} 

@article{Nosek_Errington_2017, title={Making sense of replications}, volume={6}, url={http://dx.doi.org/10.7554/eLife.23383}, DOI={10.7554/elife.23383}, abstractNote={<jats:p>The first results from the Reproducibility Project: Cancer Biology suggest that there is scope for improving reproducibility in pre-clinical cancer research.</jats:p>}, journal={eLife}, publisher={eLife Sciences Publications, Ltd}, author={Nosek, Brian A and Errington, Timothy M}, year={2017}} 

@article{chanock_2007, title={Replicating genotype–phenotype associations}, volume={447}, url={http://dx.doi.org/10.1038/447655a}, DOI={10.1038/447655a}, number={7145}, journal={Nature}, publisher={Springer Science and Business Media LLC}, year={2007},  pages={655–660} } 

@article{Prinz_Schlange_Asadullah_2011, title={Believe it or not: how much can we rely on published data on potential drug targets?}, volume={10}, url={http://dx.doi.org/10.1038/nrd3439-c1}, DOI={10.1038/nrd3439-c1}, number={9}, journal={Nature Reviews Drug Discovery}, publisher={Springer Science and Business Media LLC}, author={Prinz, Florian and Schlange, Thomas and Asadullah, Khusru}, year={2011}, pages={712–712} } 

@article{Burkner_2017, title={brms: An R Package for Bayesian Multilevel Models Using Stan}, volume={80}, url={http://dx.doi.org/10.18637/jss.v080.i01}, DOI={10.18637/jss.v080.i01}, number={1}, journal={Journal of Statistical Software}, publisher={Foundation for Open Access Statistic}, author={Bürkner, Paul-Christian}, year={2017} } 